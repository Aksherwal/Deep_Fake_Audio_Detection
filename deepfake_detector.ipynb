{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd31654b",
   "metadata": {},
   "source": [
    "# Deepfake Audio Detection Demo\n",
    "\n",
    "This notebook demonstrates a lightweight version of a hybrid deepfake audio detection system using Retrieval-Augmented Detection (RADD), GANs, and VAEs. It’s designed for a quick demo, processing only 10 real and 10 fake `.flac` files from the ASVspoof2019 LA dataset, training briefly, and running real-time detection for 10 seconds.\n",
    "\n",
    "## Prerequisites\n",
    "- **Dataset**: ASVspoof2019 LA (`ASVspoof2019_LA_train/flac/` and `ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt`) in the project directory.\n",
    "- **Environment**: Python 3.9 with dependencies installed (`librosa`, `numpy`, `faiss-cpu`, `transformers`, `torch`, `tensorflow`, `pyaudio`).\n",
    "- **Hardware**: Microphone for real-time detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c761db2",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a928ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Define Constants\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "import faiss\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "import pyaudio\n",
    "import mysql.connector\n",
    "import hashlib\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_SIZE = 2048  # Increased to match n_fft\n",
    "LATENT_DIM = 100\n",
    "SPECTROGRAM_SHAPE = (1025, 87)  # 1-second audio at 16kHz with n_fft=2048\n",
    "DATA_DIR = 'flac'\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"mysql-1af4031e-audiodeepfake.h.aivencloud.com\",\n",
    "    \"port\": 12094,\n",
    "    \"user\": \"avnadmin\",\n",
    "    \"password\": \"AVNS_qweb09E825R-UKRWQBR\",\n",
    "    \"database\": \"defaultdb\"\n",
    "}\n",
    "DEMO_FILES = 100  #for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a84d7",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preprocess Audio Data\n",
    "Load 100 real (bonafide) and 100 fake (spoof) .flac files using the protocol file to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f1e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:31:13,237 - INFO - Loading dataset for demo...\n",
      "2025-04-05 22:31:14,927 - INFO - Loaded 100 real and 100 fake samples.\n",
      "2025-04-05 22:31:14,928 - INFO - Real samples: 100, Fake samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Preprocess Audio Data\n",
    "def load_and_preprocess_audio(directory, label, max_files=DEMO_FILES, protocol_file=None):\n",
    "    audio_files = glob.glob(os.path.join(directory, '*.flac'))\n",
    "    spectrograms, labels = [], []\n",
    "    if protocol_file and os.path.exists(protocol_file):\n",
    "        with open(protocol_file, 'r') as f:\n",
    "            protocol = {line.split()[1]: line.split()[4] for line in f.readlines()[1:]}\n",
    "    else:\n",
    "        logging.warning(\"Protocol file not found; assuming all files match the label.\")\n",
    "        protocol = None\n",
    "    \n",
    "    file_count = 0\n",
    "    for file in audio_files:\n",
    "        if file_count >= max_files:\n",
    "            break\n",
    "        try:\n",
    "            filename = os.path.basename(file).replace('.flac', '')\n",
    "            if protocol and filename not in protocol:\n",
    "                continue\n",
    "            is_real = 0 if protocol and protocol[filename] == 'bonafide' else 1\n",
    "            if protocol and is_real != label:\n",
    "                continue\n",
    "            y, sr = librosa.load(file, sr=SAMPLE_RATE)\n",
    "            S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
    "            S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "            if S_db.shape != SPECTROGRAM_SHAPE:\n",
    "                S_db = librosa.util.fix_length(S_db, size=SPECTROGRAM_SHAPE[1], axis=1)[:SPECTROGRAM_SHAPE[0], :]\n",
    "            spectrograms.append(S_db)\n",
    "            labels.append(label)\n",
    "            file_count += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file}: {e}\")\n",
    "    return np.array(spectrograms), np.array(labels)\n",
    "\n",
    "logging.info(\"Loading dataset for demo...\")\n",
    "protocol_path = 'ASVspoof2019.LA.cm.train.trn.txt'\n",
    "real_spectrograms, real_labels = load_and_preprocess_audio(DATA_DIR, 0, protocol_file=protocol_path)\n",
    "fake_spectrograms, fake_labels = load_and_preprocess_audio(DATA_DIR, 1, protocol_file=protocol_path)\n",
    "all_spectrograms = np.concatenate([real_spectrograms, fake_spectrograms])\n",
    "all_labels = np.concatenate([real_labels, fake_labels])\n",
    "logging.info(f\"Loaded {len(real_spectrograms)} real and {len(fake_spectrograms)} fake samples.\")\n",
    "logging.info(f\"Real samples: {np.sum(all_labels == 0)}, Fake samples: {np.sum(all_labels == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c45f",
   "metadata": {},
   "source": [
    "### Step 3: Set Up Retrieval-Augmented Detection (RADD)\n",
    "Extract features with Wav2Vec2 and index them with FAISS for similarity retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d898cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\audiodeepfake\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "e:\\audiodeepfake\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Set Up Retrieval-Augmented Detection (RADD)\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base')\n",
    "model = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base')\n",
    "\n",
    "def extract_features(audio):\n",
    "    inputs = processor(audio, return_tensors='pt', sampling_rate=SAMPLE_RATE, padding=True)\n",
    "    with torch.no_grad():\n",
    "        features = model(inputs.input_values).last_hidden_state\n",
    "    return features.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "audio_samples = [librosa.istft(s) for s in all_spectrograms]\n",
    "features = np.array([extract_features(audio) for audio in audio_samples])\n",
    "index = faiss.IndexFlatL2(features.shape[1])\n",
    "index.add(features)\n",
    "\n",
    "def retrieve_similar(new_audio, k=5):\n",
    "    new_features = extract_features(new_audio).reshape(1, -1)\n",
    "    distances, indices = index.search(new_features, k)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa2208",
   "metadata": {},
   "source": [
    "### Step 4: Train GAN for Synthetic Deepfakes\n",
    "Train a GAN with 50 epochs to generate synthetic deepfake samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e5e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:32:02,347 - INFO - Training GAN for demo...\n",
      "e:\\audiodeepfake\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n",
      "2025-04-05 22:32:03,283 - INFO - GAN Epoch 0: D Loss: 1.2657653093338013, G Loss: 0.6925024390220642\n",
      "2025-04-05 22:32:05,454 - INFO - GAN Epoch 10: D Loss: 2.823486328125, G Loss: 0.4016060531139374\n",
      "2025-04-05 22:32:07,557 - INFO - GAN Epoch 20: D Loss: 2.843430995941162, G Loss: 0.37007731199264526\n",
      "2025-04-05 22:32:09,650 - INFO - GAN Epoch 30: D Loss: 2.7539262771606445, G Loss: 0.35752567648887634\n",
      "2025-04-05 22:32:11,786 - INFO - GAN Epoch 40: D Loss: 2.7970542907714844, G Loss: 0.3510686457157135\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train GAN for Synthetic Deepfakes\n",
    "def build_generator():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(LATENT_DIM,)),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dense(np.prod(SPECTROGRAM_SHAPE), activation='tanh'),\n",
    "        layers.Reshape(SPECTROGRAM_SHAPE)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=SPECTROGRAM_SHAPE),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(LATENT_DIM,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
    "\n",
    "def train_gan(epochs=50, batch_size=8):\n",
    "    for epoch in range(epochs):\n",
    "        noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "        fake_spectrograms = generator.predict(noise, verbose=0)\n",
    "        real_idx = np.random.randint(0, all_spectrograms.shape[0], batch_size)\n",
    "        real_spectrograms = all_spectrograms[real_idx]\n",
    "        X = np.concatenate([real_spectrograms, fake_spectrograms])\n",
    "        y = np.array([0.9] * batch_size + [0.1] * batch_size)  # Label smoothing\n",
    "        d_loss = discriminator.train_on_batch(X, y)\n",
    "        noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "        y_gen = np.ones(batch_size) * 0.9  # Smooth generator labels\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"GAN Epoch {epoch}: D Loss: {d_loss}, G Loss: {g_loss}\")\n",
    "\n",
    "logging.info(\"Training GAN for demo...\")\n",
    "train_gan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6d266",
   "metadata": {},
   "source": [
    "### Step 5: Train VAE for Data Augmentation\n",
    "Train a VAE with 10 epochs to augment real audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4ff14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:32:13,899 - INFO - Training VAE for demo...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train VAE for Data Augmentation\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class VAE(models.Model):\n",
    "    def __init__(self, spectrogram_shape, latent_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.spectrogram_shape = spectrogram_shape\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_inputs = layers.Input(shape=spectrogram_shape)\n",
    "        x = layers.Flatten()(self.encoder_inputs)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        self.z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "        self.z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "        self.z = layers.Lambda(self._sampling, output_shape=(latent_dim,), name='z')([self.z_mean, self.z_log_var])\n",
    "        self.encoder = models.Model(self.encoder_inputs, [self.z_mean, self.z_log_var, self.z], name='encoder')\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "        x = layers.Dense(512, activation='relu')(self.decoder_inputs)\n",
    "        x = layers.Dense(np.prod(spectrogram_shape), activation='tanh')(x)\n",
    "        self.decoder_outputs = layers.Reshape(spectrogram_shape)(x)\n",
    "        self.decoder = models.Model(self.decoder_inputs, self.decoder_outputs, name='decoder')\n",
    "\n",
    "        # VAE outputs\n",
    "        self.outputs = self.decoder(self.encoder(self.encoder_inputs)[2])\n",
    "\n",
    "    def _sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        if training:\n",
    "            reconstruction_loss = K.mean(K.square(inputs - reconstructed))\n",
    "            kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "            self.add_loss(reconstruction_loss + kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "def build_vae():\n",
    "    vae = VAE(spectrogram_shape=SPECTROGRAM_SHAPE, latent_dim=LATENT_DIM)\n",
    "    vae.compile(optimizer='adam')\n",
    "    encoder = vae.encoder\n",
    "    decoder = vae.decoder\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "vae, encoder, decoder = build_vae()\n",
    "logging.info(\"Training VAE for demo...\")\n",
    "vae.fit(all_spectrograms[all_labels == 0], epochs=10, batch_size=8, verbose=0)\n",
    "\n",
    "noise = np.random.normal(0, 1, (len(real_spectrograms), LATENT_DIM))\n",
    "augmented_spectrograms = decoder.predict(noise, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c186fb",
   "metadata": {},
   "source": [
    "### Step 6: Caching Mechanism\n",
    "Cache retrieval results for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f211527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Caching Mechanism\n",
    "cache = {}\n",
    "def get_cached_retrieval(audio_hash, audio):\n",
    "    if audio_hash not in cache:\n",
    "        distances, indices = retrieve_similar(audio)\n",
    "        cache[audio_hash] = (distances, indices)\n",
    "    return cache[audio_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86abe",
   "metadata": {},
   "source": [
    "### Step 7: Model Integration, Training, and Continuous Learning\n",
    "Combine all data, define and train the detector with 50 epochs, and set up continuous learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25497910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:32:47,649 - INFO - Training detector for demo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.5422 - loss: 0.9036 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 2/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 229ms/step - accuracy: 0.4650 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 226ms/step - accuracy: 0.5134 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 4/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 222ms/step - accuracy: 0.5477 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 5/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 223ms/step - accuracy: 0.4075 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 225ms/step - accuracy: 0.4902 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 246ms/step - accuracy: 0.4961 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 233ms/step - accuracy: 0.4595 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 228ms/step - accuracy: 0.5367 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 241ms/step - accuracy: 0.4461 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 11/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 237ms/step - accuracy: 0.5601 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 12/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 250ms/step - accuracy: 0.5042 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 13/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 236ms/step - accuracy: 0.4812 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Model Integration, Training, and Continuous Learning\n",
    "# Generate synthetic data\n",
    "synthetic_spectrograms = generator.predict(np.random.normal(0, 1, (len(real_spectrograms), LATENT_DIM)), verbose=0)\n",
    "all_data = np.concatenate([all_spectrograms, synthetic_spectrograms, augmented_spectrograms])\n",
    "all_labels_extended = np.concatenate([all_labels, [1] * len(synthetic_spectrograms), [0] * len(augmented_spectrograms)])\n",
    "\n",
    "# Split data with stratification\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    all_data, all_labels_extended, test_size=0.2, stratify=all_labels_extended, random_state=42\n",
    ")\n",
    "train_data = train_data[..., np.newaxis]\n",
    "val_data = val_data[..., np.newaxis]\n",
    "\n",
    "# Define and train detector\n",
    "detector = models.Sequential([\n",
    "    layers.Input(shape=SPECTROGRAM_SHAPE + (1,)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "detector.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "logging.info(\"Training detector for demo...\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "detector.fit(train_data, train_labels, epochs=50, batch_size=8, validation_data=(val_data, val_labels), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Continuous learning setup\n",
    "conn = mysql.connector.connect(**DB_CONFIG)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS samples (\n",
    "        hash TEXT,\n",
    "        spectrogram BLOB,\n",
    "        label INT,\n",
    "        PRIMARY KEY (hash(64))\n",
    "    )\n",
    "''')\n",
    "\n",
    "def add_new_sample(spectrogram, label):\n",
    "    spectrogram_blob = spectrogram.tobytes()\n",
    "    hash_value = hashlib.md5(spectrogram_blob).hexdigest()\n",
    "    cur.execute('INSERT INTO samples (hash, spectrogram, label) VALUES (%s, %s, %s)', \n",
    "                (hash_value, spectrogram_blob, label))\n",
    "    conn.commit()\n",
    "\n",
    "def retrain_model():\n",
    "    cur.execute('SELECT spectrogram, label FROM samples')\n",
    "    rows = cur.fetchall()\n",
    "    new_spectrograms, new_labels = [], []\n",
    "    for row in rows:\n",
    "        spectrogram = np.frombuffer(row[0], dtype=np.float32).reshape(SPECTROGRAM_SHAPE)\n",
    "        new_spectrograms.append(spectrogram)\n",
    "        new_labels.append(row[1])\n",
    "    if new_spectrograms:\n",
    "        new_data = np.array(new_spectrograms)[..., np.newaxis]\n",
    "        new_labels = np.array(new_labels)\n",
    "        detector.fit(new_data, new_labels, epochs=2, batch_size=8, verbose=0)\n",
    "        logging.info(\"Model retrained with new samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f680127",
   "metadata": {},
   "source": [
    "### Step 8: Real-Time Detection\n",
    "Run real-time detection for 10 seconds using the microphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e966bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:34:54,366 - INFO - Starting real-time detection (10 seconds for demo)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 59 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023D27075EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:34:54,581 - WARNING - 5 out of the last 59 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023D27075EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2025-04-05 22:34:54,703 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 65.57\n",
      "2025-04-05 22:34:54,827 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 27.66\n",
      "2025-04-05 22:34:54,964 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 24.49\n",
      "2025-04-05 22:34:55,108 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 64.52\n",
      "2025-04-05 22:34:55,247 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 23.16\n",
      "2025-04-05 22:34:55,384 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 26.39\n",
      "2025-04-05 22:34:55,524 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 70.46\n",
      "2025-04-05 22:34:55,657 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 64.28\n",
      "2025-04-05 22:34:55,800 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 78.86\n",
      "2025-04-05 22:34:55,928 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 23.21\n",
      "2025-04-05 22:34:56,074 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.63\n",
      "2025-04-05 22:34:56,214 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 34.22\n",
      "2025-04-05 22:34:56,352 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 75.47\n",
      "2025-04-05 22:34:56,482 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 18.52\n",
      "2025-04-05 22:34:56,617 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 28.56\n",
      "2025-04-05 22:34:56,764 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.16\n",
      "2025-04-05 22:34:56,899 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 41.51\n",
      "2025-04-05 22:34:57,029 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 46.10\n",
      "2025-04-05 22:34:57,215 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 23.55\n",
      "2025-04-05 22:34:57,345 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 53.93\n",
      "2025-04-05 22:34:57,490 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 27.13\n",
      "2025-04-05 22:34:57,625 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 41.51\n",
      "2025-04-05 22:34:57,776 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 74.05\n",
      "2025-04-05 22:34:57,925 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.29\n",
      "2025-04-05 22:34:58,084 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.43\n",
      "2025-04-05 22:34:58,239 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 21.21\n",
      "2025-04-05 22:34:58,399 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 18.35\n",
      "2025-04-05 22:34:58,537 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 27.83\n",
      "2025-04-05 22:34:58,675 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 17.36\n",
      "2025-04-05 22:34:58,811 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 69.34\n",
      "2025-04-05 22:34:58,945 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 29.51\n",
      "2025-04-05 22:34:59,083 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 24.16\n",
      "2025-04-05 22:34:59,218 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 76.13\n",
      "2025-04-05 22:34:59,361 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.95\n",
      "2025-04-05 22:34:59,492 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.79\n",
      "2025-04-05 22:34:59,639 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 18.39\n",
      "2025-04-05 22:34:59,777 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.29\n",
      "2025-04-05 22:34:59,924 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 23.88\n",
      "2025-04-05 22:35:00,073 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 32.20\n",
      "2025-04-05 22:35:00,221 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 40.46\n",
      "2025-04-05 22:35:00,355 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 75.26\n",
      "2025-04-05 22:35:00,489 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 13.02\n",
      "2025-04-05 22:35:00,643 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 36.64\n",
      "2025-04-05 22:35:00,784 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.71\n",
      "2025-04-05 22:35:00,930 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 58.47\n",
      "2025-04-05 22:35:01,063 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 63.77\n",
      "2025-04-05 22:35:01,193 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 78.26\n",
      "2025-04-05 22:35:01,326 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 44.62\n",
      "2025-04-05 22:35:01,469 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 77.82\n",
      "2025-04-05 22:35:01,610 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 21.96\n",
      "2025-04-05 22:35:01,745 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 33.24\n",
      "2025-04-05 22:35:01,885 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 23.97\n",
      "2025-04-05 22:35:02,014 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 17.81\n",
      "2025-04-05 22:35:02,151 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.44\n",
      "2025-04-05 22:35:02,295 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 26.92\n",
      "2025-04-05 22:35:02,436 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 36.66\n",
      "2025-04-05 22:35:02,576 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 72.03\n",
      "2025-04-05 22:35:02,713 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 71.68\n",
      "2025-04-05 22:35:02,851 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 80.09\n",
      "2025-04-05 22:35:02,985 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 54.63\n",
      "2025-04-05 22:35:03,126 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 17.45\n",
      "2025-04-05 22:35:03,262 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 75.35\n",
      "2025-04-05 22:35:03,406 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.64\n",
      "2025-04-05 22:35:03,542 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 77.51\n",
      "2025-04-05 22:35:03,690 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 26.08\n",
      "2025-04-05 22:35:03,830 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 21.05\n",
      "2025-04-05 22:35:03,971 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.29\n",
      "2025-04-05 22:35:04,105 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 41.97\n",
      "2025-04-05 22:35:04,247 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 21.57\n",
      "2025-04-05 22:35:04,381 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 71.58\n",
      "2025-04-05 22:35:04,521 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 21.20\n",
      "2025-04-05 22:35:04,664 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 72.91\n",
      "2025-04-05 22:35:04,805 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 63.62\n",
      "2025-04-05 22:35:04,943 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 20.16\n",
      "2025-04-05 22:35:05,092 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 25.18\n",
      "2025-04-05 22:35:05,229 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 42.33\n",
      "2025-04-05 22:35:05,368 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 22.24\n",
      "2025-04-05 22:35:05,513 - INFO - Deepfake Probability: 0.50, Retrieval Distance: 68.75\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Real-Time Detection\n",
    "def real_time_detection():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paFloat32, channels=1, rate=SAMPLE_RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "    logging.info(\"Starting real-time detection (10 seconds for demo)...\")\n",
    "    try:\n",
    "        for _ in range(int(10 * SAMPLE_RATE / CHUNK_SIZE)):\n",
    "            try:\n",
    "                data = stream.read(CHUNK_SIZE, exception_on_overflow=False)\n",
    "                audio = np.frombuffer(data, dtype=np.float32)\n",
    "                S = librosa.stft(audio, n_fft=2048, hop_length=512)\n",
    "                S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "                S_db = librosa.util.fix_length(S_db, size=SPECTROGRAM_SHAPE[1], axis=1)[:SPECTROGRAM_SHAPE[0], :]\n",
    "                S_db = S_db[np.newaxis, ..., np.newaxis]\n",
    "                prediction = detector.predict(S_db, verbose=0)[0][0]\n",
    "                audio_hash = hashlib.md5(audio.tobytes()).hexdigest()\n",
    "                distances, _ = get_cached_retrieval(audio_hash, audio)\n",
    "                logging.info(f\"Deepfake Probability: {prediction:.2f}, Retrieval Distance: {distances[0][0]:.2f}\")\n",
    "                if prediction > 0.7:  # Adjusted threshold\n",
    "                    add_new_sample(S_db[0, ..., 0], 1)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in stream: {e}\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "real_time_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64c9bb",
   "metadata": {},
   "source": [
    "## Step 9: Evaluation and Retraining\n",
    "Evaluate the model and retrain with new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e381261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:35:05,555 - INFO - Validation real: 40, Validation fake: 40\n",
      "2025-04-05 22:35:05,941 - INFO - Accuracy: 0.50\n",
      "2025-04-05 22:35:05,944 - INFO - Precision: 0.50\n",
      "2025-04-05 22:35:05,946 - INFO - Recall: 1.00\n",
      "2025-04-05 22:35:05,948 - INFO - F1-Score: 0.67\n",
      "2025-04-05 22:35:06,014 - INFO - Test prediction on first sample: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluation and Retraining\n",
    "logging.info(f\"Validation real: {np.sum(val_labels == 0)}, Validation fake: {np.sum(val_labels == 1)}\")\n",
    "predictions = (detector.predict(val_data, verbose=0) > 0.5).astype(int)\n",
    "logging.info(f\"Accuracy: {accuracy_score(val_labels, predictions):.2f}\")\n",
    "logging.info(f\"Precision: {precision_score(val_labels, predictions):.2f}\")\n",
    "logging.info(f\"Recall: {recall_score(val_labels, predictions):.2f}\")\n",
    "logging.info(f\"F1-Score: {f1_score(val_labels, predictions):.2f}\")\n",
    "\n",
    "# Test prediction on a known sample\n",
    "test_sample = all_data[0][np.newaxis, ..., np.newaxis]\n",
    "logging.info(f\"Test prediction on first sample: {detector.predict(test_sample, verbose=0)[0][0]:.2f}\")\n",
    "\n",
    "retrain_model()\n",
    "\n",
    "# Cleanup\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
